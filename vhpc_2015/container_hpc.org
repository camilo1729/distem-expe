#+TITLE: Performance Evaluation of Containers for HPC
#+AUTHOR: \\ \vspace{0.5cm} Cristian Ruiz, Emmanuel Jeanvoine and Lucas Nussbaum \\ \vspace{0.5cm} INRIA Nancy, France \\ \vspace{0.5cm} VHPC'15
#+EMAIL:     {Cristian.Ruiz}@inria.fr
#+DATE:

#+OPTIONS: H:2
#+BEAMER_COLOR_THEME:
#+BEAMER_FONT_THEME:
#+BEAMER_HEADER:
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+BEAMER_INNER_THEME:
#+BEAMER_OUTER_THEME:
#+BEAMER_THEME: default
#+LATEX_CLASS: beamer


#+OPTIONS:   H:2 toc:nil

#+LATEX_HEADER: \usepackage{multirow}
#+LaTeX_HEADER: \usepackage{minted}
#+LaTeX_HEADER: \usepackage{fontspec}
#+LaTeX_HEADER: \usepackage{graphicx}
#+LaTeX_HEADER: \usepackage{subcaption}
#+LaTeX_HEADER: \usepackage{color}
#+latex_header: \newminted{ruby}{fontsize=\scriptsize}
#+latex_header: \usepackage{./theme/beamerthemeCristian}
#+latex_header: \usepackage[nocolor]{./theme/beamerAlvinMacros}
#+latex_header: \usepackage[absolute,overlay]{textpos}
#+latex_header: \setlength{\TPHorizModule}{\paperwidth}
#+latex_header: \setlength{\TPVertModule}{\paperheight}
#+latex_header: \textblockorigin{0mm}{0mm}
#+LATEX_HEADER: \usepackage{natbib}
#+LATEX_HEADER: \usepackage{bibentry}
#+LATEX_HEADER: \usepackage{dirtree}
#+LATEX_HEADER: \newcommand\Fontvi{\fontsize{6}{7.2}\selectfont}
#+LATEX_HEADER: \nobibliography*
#+BIND: org-latex-title-command ""
#+BEGIN_LaTeX



\sloppy
\frame{
  \thispagestyle{empty}
  \titlepage
  \begin{center}
    \includegraphics[height=1.2cm]{logos/inr_logo_sans_sign_coul.png}
    \hspace{0.5cm}
  \insertlogo{\includegraphics[height=1.2cm]{logos/grid5000.png}}
   \hspace{0.5cm}
  \insertlogo{\includegraphics[height=1.2cm]{logos/logo_loria_complet_couleur.pdf}}
  \end{center}

}

#+END_LaTex

#+LaTeX: \tableofcontents



* Introduction

** Linux Containers

   :PROPERTIES:
   :BEAMER_OPT:
   :END:


#+BEGIN_LaTeX
\par {\usebeamerfont{title} Container based virtualization}\par
\vspace{1cm} %\hfill

#+END_LaTeX


** Containers

- *Containers* refers generally to *Operating-system-level virtualization*,
  where the *kernel* of an operating system allows for multiple isolated *user-space instances*.

#+BEGIN_LaTeX
\begin{figure}[!h]
  \center
  \includegraphics[scale=0.65]{figures/lxc-vm.jpg}
  \label{fig:hpc}
\end{figure}
#+END_LaTeX

** Implementations

- Chroot
- Linux-VServer
- FreeBSD Jails
- Solaris Containers
- OpenVZ
- Linux-Containers (LXC)

** /namesapces and cgroups/

- Both features incorporated in Linux kernel since 2006 (Linux 2.6.24).
- Several container solutions: LXC, Docker, libcontainer, systemd-nspawn.

#+BEGIN_LaTeX
\begin{figure}[!h]
  \center
\includegraphics[scale=0.30]{figures/libcontainer-diagram.pdf}
  \label{fig:hpc}
\end{figure}
#+END_LaTeX

# /libcontainer/ *will become the standard to manage containers*



* State of the art
** Virtualization solutions for HPC

- Youssef et al\cite{Youseff:2006:EPI:1308175.1308346} evaluated Xen using HPC
  Challenge benchmarks and LLNL ASC Purple benchmarks.

- Nussbaum et al\cite{nussbaum2009linux} compared Xen and KVM using
  micro-benchmarks and the HPC Challenge benchmarks.

- Regola et al\cite{regola2010recommendations} focuses on the I/O
  performance of Xen, KVM and OpenVZ.

- Public Cloud platforms: Amazon EC2 \cite{5353067} and Microsoft Azure\cite{Tudoran:2012:PEA:2168697.2168701}
  have been evaluated using Intel MPI benchmarks and scientific applications.

** Container performance evaluation

- Matthews et al\cite{matthews2007quantifying} compared the performance of VMWare,
  Xen, Solaris containers and OpenVZ using custom benchmarks.
- Felter et al\cite{ibmtrdocker} evaluated the I/O performance of Docker using MySQL,
  Linpack, Stream, RandomAccess, nuttcp, netperf, fio, and Redis.
- Walter et al\cite{4482796} compared VMWare Server, Xen and OpenVZ using NetPerf, IOZone, and the NAS Parallel Benchmarks.

- Xavier et al\cite{6498558} compared Linux VServer, OpenVZ,
  LXC and Xen using the HPC Challenge benchmarks and the NAS
  Parallel Benchmarks.

** In this work, we answer:

   :PROPERTIES:
   :BEAMER_OPT:
   :END:



- What is the overhead of oversubscription using different versions of Linux kernel?
- What is the performance of inter-container communication?
- What is the impact of running an HPC workload with several MPI processes inside containers?



* Experimental evaluation

** Experimental setup

*** Hardware
- Cluster in Grid'5000 Testbed\cite{grid5000} where each node is equipped with two Intel Xeon E5-2630v3 processors (with 8 cores each), 128 GB of RAM and
  a 10 Gigabit Ethernet adapter.
- Our experimental setup included up to 64 machines.

*** Software
- Debian Jessie, Linux kernel versions: 3.2, 3.16 and 4.0, OpenMPI and NPB.
  We instrumented the benchmarks: LU, EP, CG, MG, FT, IS using TAU.
# \cite{Shende06thetau}.
- We automate the experimentation processes using Distem\footnote{https://distem.gforge.inria.fr}
  and Kameleon\footnote{https://github.com/camilo1729/distem-recipes}.


** Network setup

- *Veth pair + Linux brigde*
- Veth pair + OpenvSwitch
- MACVLAN
- Phys

#+BEGIN_LaTeX
\begin{figure}[!h]
  \center
  \includegraphics[scale=0.4]{figures/lxc-veth.pdf}
  \label{fig:hpc}
 % \caption{VETH network}
\end{figure}
#+END_LaTeX


** Linux kernel version

   32 containers running on: 8,16,32 physical machines.

*** Overhead introduced 				      :B_block:BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.5
    :BEAMER_env: block
    :END:


- 3.2: *1577.78%*.
- 3.16: *22.67%*.
- 4.0: *2.40%*.
- Overhead present in MPI communication.
- Since Linux kernel version *3.11*, *TSO* was enabled in *veth*.
*** image							      :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.5
    :END:


# *** notes of results						   :noexport:

# This notes explain the results obtained

# The execution with kernel 3.2 of 2 container per node takes 15 times more than native
# communication time is really degradated, cpu is not affected.

#+BEGIN_LaTeX
\begin{figure}[!h]
  \center
  \includegraphics[scale=0.32]{figures/execution_time-kernel-cgB.pdf}
  \label{fig:hpc}
  \caption{CG.B}
\end{figure}
#+END_LaTeX

** Oversubscription Linux kernel 4.0

- 64 containers running over: 8,16,32,64 physical machines.
- There is a \textit{veth} per MPI processes.
*** Results 						      :B_block:BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.5
    :BEAMER_env: block
    :END:

- Top 3 worst performance results: MG, FT, LU.
- Maximum overhead (15%, 67%).
  # the highest overhead was obtained with MG benchmark class
  # B which sends around 1.22 GBytes during its execution.

# there is not significant difference between running 1 or 2 container per physical machine.

# Notes:
# total overhead with 4 and 8 container per machine

# CG:
# 64 1/node 24%
# 16 4/nodes 20%
# 8  4/nodes 21%
# 8  8/nodes 15%

# Total: 80%, GBytes Received: 0.985 GB

# MG:

# 8 4/nodes 49%
# 16 4/nodes 47%
# 8 8/nodes 67%

# Total: 166%, GBytes Received: 0.369 GB

# LU:

# 8 4/nodes 20%
# 16 4/nodes 34%
# 8 8/nodes 50%

# Total: 104%, 0.345 GB

# EP:

# 8 4/nodes 28 %
# 16 4/nodes 28 %
# 8 8/nodes  27%

# Total: 83%, performnace issues with the recent version of kernel

# FT:
# 8 4/nodes 47%
# 16 4/nodes 43%
# 8 8/nodes 62%

# Total: 152% , GBytes Received: 1.221 GB

# IS:

# 8 4/nodes 32%
# 16 4/nodes 19%
# 8 8/nodes 37%
# Total: 88%, GBytes Received: 0.620 GB

*** image							      :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.5
    :END:


#+BEGIN_LaTeX
\begin{figure}[!h]
  \center
  \includegraphics[scale=0.30]{figures/execution_time-tso-40.pdf}
  \label{fig:hpc}
  \caption{FT.B}
\end{figure}
#+END_LaTeX

** Inter-container communication

-  /container/ and /SM/: 1 physical node.
-  /native/ : 2, 4, 8 physical nodes.

All running the equivalent number of MPI processes.

#+BEGIN_LaTeX
\begin{figure}[H]
  \centering
\begin{subfigure}[b]{0.42\textwidth}
    \includegraphics[scale=0.25,angle=0]{figures/inter-container-mgC.pdf}
    \caption{MG Class B}
  \end{subfigure}
  \begin{subfigure}[b]{0.42\textwidth}
    \includegraphics[scale=0.25,angle=0]{figures/inter-container-isC.pdf}
    \caption{IS Class C}
  \end{subfigure}
\end{figure}
#+END_LaTeX

** Inter-container communication

#+BEGIN_LaTeX
\begin{table}
  \scriptsize

\input{inter-container-table.tex}

\caption{Profile results. Time in \textit{msec}}
%\label{tab:benchprofiles}

\end{table}
#+END_LaTeX


- Inter-container communication is the fastest.
- Important degradation of the CPU performance for memory bound applications.
- LU: 53%, MG: 53%, EP: 25%, CG: 12%, FT: 0%, IS: 0% (overheads regarding native)

** Multinode inter-container communication

- 16 MPI processes were run per physical machine or container
- We used a maximum of 32 physical machines.
#+BEGIN_LaTeX

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.42\textwidth}
    \includegraphics[scale=0.25,angle=0]{figures/veth_overhead-tso-cgB.pdf}
    \caption{CG Class B}
  \end{subfigure}
  \begin{subfigure}[b]{0.42\textwidth}
    \includegraphics[scale=0.25,angle=0]{figures/veth_overhead-tso-ftB.pdf}
    \caption{FT Class B}
  \end{subfigure}
\end{figure}

#+END_LaTeX

** Multinode inter-container communication

- Benchmarks with low  MPI communication: we observed a maximum overhead of *5.97\%* (with *512 MPI processes*).
- Benchmarks with an intensive MPI communication: we observed a higher overhead starting from *30\%* for the benchmark LU.

- CG reaches *180\%* of overhead when *128* MPI processes are used.
  This benchmarks sends a high number of MPI messages, around
  a 1000 times more than the first group of benchmarks
  which increase network congestion and leads to TCP timeouts.

** Multinode inter-container communication


- It was shown how network bound applications can be severely affected by
  the default container network interconnection.

- We found a way to alleviate the overhead
  by tweaking parameters of the Linux network stack.

  - TCP minimum retransmission timeout (RTO).
  - TCP Selective Acknowledgments (SACK).


* Conclusions
** In this work ...

- We study the impact of using containers in the context of HPC research.

- We evaluate two interesting uses of containers in the context of HPC research: portability of complex software stacks
  and oversubscription.

- We carried out the evaluation under a configuration expected to be found in an HPC context.

** What did we find?

- The limits of using containers.
- The type of application that are affected the most.
- The level of oversubscription containers achieved without impacting considerably the application performance.
- The technology is getting mature and performance issues are being addressed through the constant evolution of the Linux kernel.


** Future work

- Measure the impact of using containers on disk I/O and other
  containers features like memory limitation.

- The overhead observed could be diminished by integrating
  more advance network interconnection such as Linux's /macvlan/, SR-IOV or OpenvSwitch\footnote{http://openvswitch.org/}.

** The end

   :PROPERTIES:
   :BEAMER_OPT:
   :END:

#+BEGIN_LaTeX
\vspace{3cm}
\par {\usebeamerfont{title} {\center Thank you} }\par
\vspace{3cm}\hfill

#+END_LaTeX



* Bibliography
** Bibliography

#+BEGIN_LaTeX

\bibliography{distem_validation.bib}
\bibliographystyle{plain}
\appendix
#+END_LaTeX
